{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20Time",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcormac/20Time/blob/master/20Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LKp64uclZFkE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**This is the very beginning of the code. To analyze our data, we will be using TensorFlow and Keras, which are APIs for machine learning that can both have high levels of obstraction. High levels of obstraction means that all I need to know is data goes into this \"black box\" and comes out with a trained model that can be used. You don't need to understand exactly how it works, just that it does.**\n",
        "\n",
        "**To run this yourself, first make a copy of this document. Then, go through each step of code and click the play button on the left hand side that you see when you hover over each section. Follow prompts from the instructions with each block of code. **"
      ]
    },
    {
      "metadata": {
        "id": "NNLI32xOU-oW",
        "colab_type": "code",
        "outputId": "445b8ac5-5fa5-4bb0-a07f-ae4b13374f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YeNRq7wndanA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The next cell is a lot of variables you can tweak to change the model.**"
      ]
    },
    {
      "metadata": {
        "id": "O3rN5e-DdYa6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of times you want to have the model read the entirety of your data\n",
        "# It is recommended that you keep this number low for times sake\n",
        "# If you are running it remotely (which is the default and what I recommend)\n",
        "# It takes about 1.5 hours per epoch\n",
        "EPOCHS=2\n",
        "# The embedding dimension \n",
        "# Haven't experimented much with this\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "# Haven't experimented much with this\n",
        "rnn_units = 1024\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JJGriVjZlR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**This next bit of code will ask you to upload a .txt file that will be used to train the model**.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Please only upload one file!**\n"
      ]
    },
    {
      "metadata": {
        "id": "9OPjIxCrk_DP",
        "colab_type": "code",
        "outputId": "c66fad26-bcc9-4dfe-dd4b-d43ab727fbe2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "text = \"\"\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  data_path = fn\n",
        "  with open(data_path, 'r') as file:\n",
        "    text = file.read().lower();\n",
        "    \n",
        "# Stats of characters in the file\n",
        "print ('{} characters'.format(len(text)))\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-626c3ea2-4fef-4033-aa17-661b4fca5ca5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-626c3ea2-4fef-4033-aa17-661b4fca5ca5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving OfficeS5E13.txt to OfficeS5E13.txt\n",
            "User uploaded file \"OfficeS5E13.txt\" with length 32281 bytes\n",
            "32281 characters\n",
            "47 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Ve7lCQ3H5Xo",
        "colab_type": "code",
        "outputId": "82a3fdd7-90c7-48ba-a408-b23f0b96f9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '0' :   9,\n",
            "  '1' :  10,\n",
            "  '2' :  11,\n",
            "  '4' :  12,\n",
            "  '5' :  13,\n",
            "  '6' :  14,\n",
            "  '8' :  15,\n",
            "  '9' :  16,\n",
            "  ':' :  17,\n",
            "  '?' :  18,\n",
            "  '[' :  19,\n",
            "  ...\n",
            "}\n",
            "dwight: last week i gave a fire safety talk. [clears throat] and nobody paid any attention. it's my own fault for using powerpoint. powerpoint is boring. people learn in a lot of different ways, but experience is the best teacher. [lights a cigarette\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-GSWNaSXeGQr",
        "colab_type": "code",
        "outputId": "57fa54c3-3d8e-409d-dfc0-18cd076a0b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices, and for indices to characters\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "print(text[:50])\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "jBOgAdH8YHbm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The cell below will make the model but not start training it.**"
      ]
    },
    {
      "metadata": {
        "id": "8sSRkPnzatDG",
        "colab_type": "code",
        "outputId": "e1113575-7135-4bcf-ea83-9406dc0ab4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in characters\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True, \n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab), \n",
        "  embedding_dim=embedding_dim, \n",
        "  rnn_units=rnn_units, \n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1): \n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()\n",
        "example_batch_loss  = tf.losses.sparse_softmax_cross_entropy(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy())\n",
        "model.compile(\n",
        "    optimizer = tf.train.AdamOptimizer(),\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 47) # (batch_size, sequence_length, vocab_size)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           12032     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 47)            48175     \n",
            "=================================================================\n",
            "Total params: 3,998,511\n",
            "Trainable params: 3,998,511\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Prediction shape:  (64, 100, 47)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       3.8493035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5WtyF5ORYaxR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**This cell will actually start the training process. Be warned, it will likely take about 1.5 hours for each epoch you do.**"
      ]
    },
    {
      "metadata": {
        "id": "puRTXEIYdO1O",
        "colab_type": "code",
        "outputId": "d2b58e06-fb29-4b31-8787-309604a39d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])\n",
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "32281/32281 [==============================] - 5056s 157ms/step - loss: 0.1719\n",
            "Epoch 2/5\n",
            "32281/32281 [==============================] - 5112s 158ms/step - loss: 0.5497\n",
            "Epoch 3/5\n",
            "32281/32281 [==============================] - 5090s 158ms/step - loss: 0.5335\n",
            "Epoch 4/5\n",
            "32281/32281 [==============================] - 5083s 157ms/step - loss: 0.2826\n",
            "Epoch 5/5\n",
            "32281/32281 [==============================] - 5090s 158ms/step - loss: 0.1298\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            12032     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 47)             48175     \n",
            "=================================================================\n",
            "Total params: 3,998,511\n",
            "Trainable params: 3,998,511\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kfVltRb9Wnuh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The next cell will make a new script based off the training and let you download it. If you run it twice, it will give you two different scripts. The three variables at the top of the block can be changed to whatever you want.**"
      ]
    },
    {
      "metadata": {
        "id": "PHmG0OuSp35w",
        "colab_type": "code",
        "outputId": "40696b11-e380-4165-a72f-b3da79cc74e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2298
        }
      },
      "cell_type": "code",
      "source": [
        "# Number of characters to generate when making predictions at the end\n",
        "# This is how long you want your output text to be\n",
        "num_generate_var = 10000\n",
        "# You can change the start string to experiment, make it relevant to your data\n",
        "start_string_var = \"dwight: \"\n",
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "temperature_var = 2.0\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  # This step uses a lot of parameters defined in cell number 2. You can change them there and run that cell again to change the generation\n",
        "  # Number of characters to generate\n",
        "  num_generate = num_generate_var\n",
        "\n",
        "  # You can change the start string to experiment\n",
        "  start_string = start_string_var\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing) \n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temperature_var\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "final_predictions = generate_text(model, start_string=start_string_var)\n",
        "with open('output.txt', 'w') as f:\n",
        "  f.write(final_predictions)\n",
        "files.download('output.txt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dwight: i didn't th: clap, music loud, fire!\n",
            "dwight: n-no one working ineck?\n",
            "jim: are you-- [peoplearned? [pause] he was aguy: jerks. just kidding, not should haropped ould have died, be a roast chael: [toby trying]\n",
            "darryl: ropped outs on high-rise.\n",
            "dwight: [sing.\"\n",
            "dwight: this is ah.. nope, wayou, but i knowng hele has thatst of our everybody, i'm going on?! [thstake] hesty, like, am: what?\n",
            "michael: what?\n",
            "jim: yes honess hones... \n",
            "michael: expractice dummy]\n",
            "dwight: michael!\n",
            "dwight: wah... yes. we at's how... yllis: oh yebegins to be nide got ice dummake out?\n",
            "michael: i am not a neck?\n",
            "jim: are you alright?\n",
            "ough, aved tooah. and now, a mont?\n",
            "michael: dwight: last weed: [to rosping] sho. [whigr dows towl: okay with the lasghing, roast! ah. ahe? 911. [move dummeredith pau a heart atter mily. \n",
            "rks. just kiddwight: i kid, you kn ist twil.. nope, we? [pul, no only right lesty, laughte'sing.\"\n",
            "jim: gounny cigarettew caw.. nop to excity: get out of the site as petrifie.\n",
            "michael! [dwightad fill flower] checks? what? [ock on im.\n",
            "dwight: that's practice du?\n",
            "michael: yes, yes i do. i love that wassers very. nain make this is gointe'r the michael.\n",
            "ming got tah. anst torainexite a looked bositt plact that? [beeve do not to arms or legs is basicalyel jey: [movie] sam i'm okay. i'looked so geight [flaughing] far. alrightake fundreah. [they hug]\n",
            "dwight toneedyou, but icture gayer than oscar. boom. roasted. [stanley faled art huckroat]\n",
            "pamethis, speact them. [andy is pace. [clears throat]\n",
            "paint are you-- [peoplould just roffichael: i mean, michael scott. [laughter]\n",
            "angela: i nor. hought breakfast. unley ad films iller. 'cause the quote was, cut offre, like, movied.\n",
            "darryl: what's man.\n",
            "pow... [hands making gunds, gumby has very realistic ng to die.\n",
            "cpretrified.\n",
            "rose: okay! \n",
            "everyone: [mur'm okenles] inter, [looks away]\n",
            "jim: rofor. a what? that's not in office use, we're no arms ok like learnda sucks. jim's been great. but i'm gonna realsing.\"\n",
            "dwight: this it wayou, but i just want iposs. i h syllis! peown]\n",
            "jim: several.\n",
            "michael: oh my god! \n",
            "dwight? chout, \"kinda sucks. jim's ight after all. looks like he's going to condy do, but i eeeds office, is your name we have you, butle movie there to work thinking, \"oh, this. y as are dead.\n",
            "dwight: [be instructions once] icture of my bare boming whoeversi consider my: gemake out with him about mebegins to grab ks away]\n",
            "jim: fire, like, mopoapam: what?\n",
            "michael: why youroast you?\n",
            "michael: sir dee--e.\n",
            "jim: wel joke's sh the faicmakes. i take mead, wnavid.\n",
            "darryl: royllis: oh my gonning.\"\n",
            "dwight: thiks away. \n",
            "sam, ll, looked soah. and now, a stay calm. \n",
            "dwight: whoor. he never betor. he n't hold back. i what? [opens her eyes, michael itarms ok is wng and donersor no sign in a smeredith: mriposying the sact thinks. justrucklaugh. [grin]\n",
            "angela: if you ever coaplanning. are dead.\n",
            "dwight: oh, that's hort... \n",
            "oscar't for stress. and now, a ks a family. \n",
            "stanley and spression tonecond in hadrenaline to sharpen you, music loud, women warm, beving up, clowing reveryonous honest did you hight. [clears throat ag] out of light after all. looks, would you say to relate moreveryone structor comes along with its the last sounds poss, pumps provere gayer thank younds, um, first up phyllis ondy does, so worteah. [they cignyone: [groaning] oh my gounny saved thinkroast!\n",
            "dwight: wayone noticiuse, a dream.\n",
            "kesing.\"\n",
            "dwight: michael: michael, fire!\n",
            "dwight: shigh-rise. no, i anda stutter?\n",
            "michael: yes. \n",
            "dwight: come on.\n",
            "michael: idnec. albert habandy: [fault?\n",
            "michael: you t have...\n",
            "michael: th: i, um, first up phis howight, befaul?\n",
            "jim: hoallat. the smoke cuse, a woodchippl.\n",
            "michael: [throws a roast! of michael scotchael: hey, y. 'cause the quote wa\n",
            "rose: okay. uh-uh--\n",
            "andy: [taved, you--\n",
            "michael: take heed of. \n",
            "dwightle furs. i widwight: [clears thrughing] of. \n",
            "dwight: and, well, lets gena\n",
            "rose: okay! \n",
            "everyour, as said \"ide o divo, you--\n",
            "michael: tat's his name?\n",
            "michael: dwight: [ learnlowinstand puicks and don ok, no, too!\n",
            "michael: okay, okay. i'lle, and in my my don't short i'll cho. boss, when--\n",
            "andy: hmm? who? sam?\n",
            "pam you might beave [ever fll of your life with. i guess he's never felooked so good in a perm and shoulder pam: so he doesn't shaving up. lilly... [pause tryinguy. no lilly stop! i don't re ga real re ally knows, but pam's parents are youch the houghing, and evesty, laughter, and comedy. icicle day, stutter?\n",
            "jimsuce. get you?\n",
            "michael: yes, yes i do. mossid wallace: could you the confur life possibly think this is very seeds broast of on the l, lets get breakfast?\n",
            "jim: hen nobody would here my gu a ride hou sactly wai kid, um, first ups. 'cause them. [and a star incredibly far away. welcome wrote hiring?\n",
            "jim: yep. you're backnow pou ming?\n",
            "jim: [pow... [hands makl y you're good to t of life do averah... n office as sup? [pam: wh, a duh, thank you. [pause] he was pr tim: n office i' uys, there copcpr trainer: be gave tock of back to al: okay, ttll you fall. they cpr t be a let's goes yould have died, becould he's stropped outof room, and about of my standing diroas poapractice during and donan style in a dress expoim.\n",
            "micture of my bare boss mant!\n",
            "dwight: wowey: chocolaterybody's stage]\n",
            "michaendy, he?\n",
            "jim: how m taking or in the looks leaved toock on a window, he? 911. [whispid ks away]\n",
            "jim: fifty percent of marriages end ih that. 'cause themake outin: ng and found of chael: hey, ya know what? forgive mrs. you're's sps here them. [andy is canyllis! people, humandy: stanley. \n",
            "michael: are yhout, \"fire!\", can youts please? oh, i can never tructorang and playing the meain acce crime.\n",
            "jessir, and insider[st that i kidg, e just ks weact her syl: idneassest. 'cause option to, \"fire!\". noandy: stanley. \n",
            "michael: ah, ah, ah, ah, ah, ah, consider we looked so gooddid you, work and then they cpr trainer: would you liket. es! [dwight your had thes... \n",
            "michael, everybod help!\n",
            "angela: pull life have...\n",
            "michael: that shought we should have cpr trainer't holder that is attachedid you say to my dad?\n",
            "jim: what?\n",
            "pam: afterrn f th what? [angela take he's ne right heres] it's oky about shout, \"fire!\", said we into gabe i can't you might be...\n",
            "group: michael scott! [laughter]\n",
            "osinny beving u a roffice, in the kid. 'll haim.\n",
            "michael: dwight your'e suppracticre: i didneck, stay ghigh-rise. what's to figuy. in shove pam: whands michael me wight: y petrified.\n",
            "rose: no, it's--ah, ah, ah, ah, st: i had a hes. wk i have a fire lashes. i hadrent you, films illeg too!\n",
            "michael: ome got that persing the smoke comeedskiad. andy: s please i we go.\n",
            "stanley: l: okay! [throws cat into chael sto smallause, ughing, on almost all seeds] uh, sunny measure you from anda\n",
            "rose: ok, alright.\n",
            "michael: okay. [walks to the window, sighs] nt you to go to work thinking? that's how i have a fire not real. this. 'm okay is the smone for no debate. you are going to small flew west for the win. nley: you're hone] ohmmm... esdid your of light. but hene. i'm notlood ifau a feel lilly so lunds, gumby has not a big deal.\n",
            "rose: no, ror. houchael: hm, been for an apambeckethis, so, uh, o spence cream. what kind do you want? [beep] no! dwight: [los prepared] save boim.\n",
            "michael: dwight your'e sunny day: [clearn]\n",
            "jim: [everyone is honestructor comestay can. i'll joke. [pam stayin' alive, s accen off e? oh?\n",
            "der hand] whand 'cause he s and stunny cigarette, and your esty, lauys, they're make ou, but i just wandstrughing, and everybody's hugging eack is your life do steps back, and starence m cready, you know we have a boomare very, ving uch thin sould go. y, you f the sing.\"\n",
            "dwight: this i, uh-uh...\n",
            "darryl: no t's next?\n",
            "michael: don't run.\n",
            "dwight: it-- it's warm.\n",
            "dwight: well, uh, and planning.] oh, i shoute tt have had the pace e'ssesssing the sing to the ne too exercise.\n",
            "michael: uh, dwight: dwight: [clears throat]\n",
            "pam: [clears th floone] some lilly: i uys, they're makes me them. [andy is the slep ride them ts closs looked sonscior no. \n",
            "stanley: [flashback] no way. uh-uh. \n",
            "stanley: [flashback]\n",
            "pam: about 911. [michaen t habit of standing to have slearn in bowl, it'st u a run.\n",
            "darryl: what? can't you, buting over hel: electricity.\n",
            "dwight: shbuch the handle. if it's his name?\n",
            "michael: stand! [throws ca big dead. no debator. he never does any work himself. even mokyllis: okays, they're becounds, gumby has lilled hr, andy: jim mouth for ther hand] whoa thercent a star incrks. that's going the hirinstructions on you're good te into garba.. no dibly things! [throws cat's hisnda sucks. i wite a be...\n",
            "dwight: he is.\n",
            "michael: ye should have cream. wha hid wight: yeah, having creed, you're tahes. i nst te coneck?\n",
            "jim, ok, this is w ce. [whispow.\n",
            "pam: ain 100 booked dwight: lilly stop! i don't care what my fair apalass \"firence assesses... \n",
            "michael. now, take noticrught your e?\n",
            "jim: hey, you crusho, firfuly bossingng the ndodid kelly. i'm down of life do too!\n",
            "michael: stayim mouth too!\n",
            "michael: oscar, sunnyone: [groaning] oh my god.\n",
            "dwight: not a viable option.\n",
            "pam: that one have you could hed. hes. ime's souppam: so he does, soffice, in the mething nice about the right lighter dube, fire!\n",
            "dwight: ohere's stunny: get ts hones. i hand]work, needs to get covere's stress sttleraffice, in the pasing.\"\n",
            "dwight: this is againe? 911. [michamake that hands michael, i don't guess 're, lilly. i office, in the past, i have...\n",
            "michael: that is the smoke] doey: can yllis: oh, ok. uh, ovenough t meredith throom, and aghing here's stressing the parking wherem. \n",
            "dwight: speakinley: [flashback] boss, wh... yep.\n",
            "dwight: hael: openg and put it over his back. you might ke? [pause] he was des! [dwight pulls fles] it, beciuse, a woodchipper, kevin, a cuch theerience is the best testrkin my hands... \n",
            "michael: idiot. \n",
            "dwight: did you shat. i will. thanks.\n",
            "dwight: it's okay. shh keep that in mindy dyone: [groaning] oh my god.\n",
            "dwight: not a 100 beats pepreveryone: [groaning] oh my: get out of the mouthe ylllis: oh, ok. [phyllis signs for there coffice almost ach the learned? [stanley fal, let's lile, acce\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xoTQWM99g3vy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Portions of this page are reproduced and/or modified from work created and [shared by Google](https://developers.google.com/readme/policies/) and used according to terms described in the [Creative Commons 3.0 Attribution License](http://creativecommons.org/licenses/by/3.0/)."
      ]
    }
  ]
}