{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20Time",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcormac/20Time/blob/master/20Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LKp64uclZFkE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the very beginning of the code. To analyze our data, we will be using TensorFlow and Keras, which are APIs for machine learning that can both have high levels of obstraction. High levels of obstraction means that all I need to know is data goes into this \"black box\" and comes out with a trained model that can be used. You don't need to understand exactly how it works, just that it does.\n",
        "\n",
        "To run this yourself, first make a copy of this document. Then, go through each step of code and click the play button on the left hand side that you see when you hover over each section. Follow prompts from the instructions with each block of code."
      ]
    },
    {
      "metadata": {
        "id": "NNLI32xOU-oW",
        "colab_type": "code",
        "outputId": "6b4c1c43-4e09-4b73-977c-15f6aefd6d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YeNRq7wndanA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next cell is a lot of variables you can tweak to change the model."
      ]
    },
    {
      "metadata": {
        "id": "O3rN5e-DdYa6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of times you want to have the model read the entirety of your data\n",
        "# It is recommended that you keep this number low for times sake\n",
        "EPOCHS=3\n",
        "# Number of characters to generate when making predictions at the end\n",
        "# This is how long you want your output text to be\n",
        "num_generate = 1000\n",
        "# You can change the start string to experiment, make it relevant to your data\n",
        "start_string_var = 'ROMEO'\n",
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "temperature = 1.0\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JJGriVjZlR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This next bit of code will ask you to upload a .txt file that will be used to train the model.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Please only upload one file!**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9OPjIxCrk_DP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = \"\"\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  data_path = fn\n",
        "  with open(data_path, 'r') as file:\n",
        "    text = file.read().lower();\n",
        "    \n",
        "# Stats of characters in the file\n",
        "print ('{} characters'.format(len(text)))\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f9OqD_CMbyb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Temp text for testing\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file).read()\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Ve7lCQ3H5Xo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices, and for indices to characters\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-GSWNaSXeGQr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBOgAdH8YHbm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/sequences/text_generation"
      ]
    },
    {
      "metadata": {
        "id": "8sSRkPnzatDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in characters\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True, \n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab), \n",
        "  embedding_dim=embedding_dim, \n",
        "  rnn_units=rnn_units, \n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1): \n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()\n",
        "example_batch_loss  = tf.losses.sparse_softmax_cross_entropy(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy())\n",
        "model.compile(\n",
        "    optimizer = tf.train.AdamOptimizer(),\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puRTXEIYdO1O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])\n",
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ZndFDmDfAh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  # This step uses a lot of parameters defined in cell number 2. You can change them there and run that cell again to change the generation\n",
        "  \n",
        "  # Converting our start string to numbers (vectorizing) \n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "final_predictions = generate_text(model, start_string=start_string_var)\n",
        "print(final_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "njQtX8ZpA9ki",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next step will download the output file to your computer\n",
        "\n",
        "**CHANGE TO OUTPUT VARIABLE NOT SPECIFIC STRING**"
      ]
    },
    {
      "metadata": {
        "id": "rd2SN8kZArSf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('output.txt', 'w') as f:\n",
        "  f.write(final_predictions)\n",
        "files.download('output.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xoTQWM99g3vy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Portions of this page are reproduced and/or modified from work created and [shared by Google](https://developers.google.com/readme/policies/) and used according to terms described in the [Creative Commons 3.0 Attribution License](http://creativecommons.org/licenses/by/3.0/)."
      ]
    }
  ]
}